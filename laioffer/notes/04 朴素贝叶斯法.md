# 第四章 朴素贝叶斯法

朴素贝叶斯（naïve Bayes）法是基于贝叶斯定理与特征条件独立假设的分类方法。

## 4.1 朴素贝叶斯法的学习与分类

### 4.1.1 基本方法

设输入空间 $\mathcal{X} \subseteq \mathbf{R}^{n}$ 为 $n$ 维向量的集合，输出空间为类标记集合 $\mathcal{Y}=\left\{c_{1},\right.\left.c_{2}, \cdots, c_{K}\right\}$。输入为特征向量$x \in \mathcal{X}$，输出为类标记(class label)。 $X$是定义在输入空间 $\mathcal{X}$ 上的随机向量，$Y$ 是定义在输出空间上的随机变量。 $P(X, Y)$是$X$和$Y$的联合概率分布。训练数据集，
$$
T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}
$$
由$P(X, Y)$独立同分布产生。

朴素贝叶斯法通过训练数据集学习联合概率分布$P(X, Y)$。具体地，学习以下先验概率分布及条件概率分布。先验概率分布
$$
P\left(Y=c_{k}\right), \quad k=1,2, \cdots, K
$$
条件概率分布
$$
P\left(X=x \mid Y=c_{k}\right)=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right), \quad k=1,2, \cdots, K
$$
于是学习到联合概率分布$P(X, Y)$。

朴素贝叶斯法对条件概率分布作了条件独立性的假设。具体地，条件独立性假设是
$$
\begin{aligned}
P\left(X=x \mid Y=c_{k}\right) &=P\left(X^{(1)}=x^{(1)}, \cdots, X^{(n)}=x^{(n)} \mid Y=c_{k}\right) \\
&=\prod_{j=1}^{n} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
\end{aligned}
$$
朴素贝叶斯法实际上学习到生成数据的机制，所以属于**生成模型**。条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。

**Bayes' theorem**公式：

$$P(A \mid B)=\frac{P(B \mid A) P(A)}{P(B)}$$

后验概率计算根据贝叶斯定理进行：
$$
P\left(Y=c_{k} \mid X=x\right)=\frac{P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}{\sum_{k} P\left(X=x \mid Y=c_{k}\right) P\left(Y=c_{k}\right)}
$$
加入独立性假设：
$$
P\left(Y=c_{k} \mid X=x\right)=\frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}, \quad k=1,2, \cdots, K
$$
这是朴素贝叶斯法分类的基本公式，于是朴素贝叶斯分类器可表示为
$$
y=f(x)=\arg \max _{c_{k}} \frac{P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}{\sum_{k} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)}
$$
注意到，在上算式中分母$c_{k}$对所有都是相同的，所有，
$$
y=\arg \max _{c_{k}} P\left(Y=c_{k}\right) \prod_{j} P\left(X^{(j)}=x^{(j)} \mid Y=c_{k}\right)
$$

### 4.1.2 后验概率最大化的含义

朴素贝叶斯法将实例分到后验概率最大的类中。这等价于期望风
险最小化。假设选择0-1损失函数：
$$
L(Y, f(X))=\left\{\begin{array}{ll}
1, & Y \neq f(X) \\
0, & Y=f(X)
\end{array}\right.
$$
算式中 $f(X)$ 是分类决策函数。这时，期望风险函数为
$$
R_{\exp }(f)=E[L(Y, f(X))]
$$
期望是对联合分布$P(X, Y)$取的。 由此取条件期望，
$$
R_{\exp }(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} \mid X\right)
$$
为了使期望风险最小化，只需对$X=x$逐个极小化，由此得到：
$$
\begin{aligned}
f(x) &=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} \mid X=x\right) \\
&=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k} \mid X=x\right) \\
&=\arg \min _{y \in \mathcal{Y}}\left(1-P\left(y=c_{k} \mid X=x\right)\right) \\
&=\arg \max _{y \in \mathcal{Y}} P\left(y=c_{k} \mid X=x\right)
\end{aligned}
$$
根据期望风险最小化原则就得到后验概率最大化准则：
$$
f(x)=\arg \max _{c_{k}} P\left(c_{k} \mid X=x\right)
$$
即朴素贝叶斯法所采用的原理。