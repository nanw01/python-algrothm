#

## 监督学习

### 基本概念

在监督学习中：

输入空间 input space：输入所有可能取值的集合
输出空间 output space：输出所有可能取值的集合

实例 instance：每个具体的输入，通常由特征向量 feature vector 表示
特征空间 feature space：所有特征向量存在的空间， 特征向量的每一维对应于一个特征

输入输出变量用大写字母表示，习惯上输入变量， $X$, 输出变量，$Y$.
输入输出变量所取的值用小写字母表示，输入变量的取值，$x$，输出变量的取值，$y$.

本书中的向量均为列向量，输入实例 $x$ 的特征向量记作
$$x=\left(x^{(1)}, x^{(2)}, \cdots, x^{(i)}, \cdots, x^{(n)}\right)^{T}$$
$x^{(i)}$表示$x$的第$i$ 个特征，$x^{(i)}$和$x_{i}$不同，本书表示多个输入变量中的第$i$ 个，即
$$x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}$$

统计学习假设数据存在一定的统计规律，$X$ 和 $Y$具有联合概率分布的假设就是监督学习关于数据的基本假设。

监督学习的目的在与学习一个有输入到输出的映射。模型属于由输入空间到输出空间的映射的集合，这个集合就是假设空间 hypothesis space。

监督学习的模型可以是概率模型或非概率模型。


## 1.3

### 模型

在监督学习过程中，模型就是所有要学习的条件概率的分布或决策函数。

模型的假设空间 hypothesis space 包括所有可能的条件概率分布或决策函数。

### 策略

risk function(风险函数) or exprcted loss(期望损失)、

给定一个训练数据集，

$$T=\left\{\left(x_{1}, y_{1}\right),\left(x_{2}, y_{2}\right), \cdots,\left(x_{N}, y_{N}\right)\right\}$$

模型$f(X)$关于训练数据集的平均损失称为经验损失 empirical risk 或经验损失 empirical loss

$$R_{\mathrm{emp}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)$$

期望风险$R_{\mathrm{exp}}(f)$ 是模型关于联合分布的期望损失，经验风险$R_{\mathrm{emp}}(f)$是模型关于训练样本集的平均损失。 

**经验风险最小化** empirical risk minimization 

$$\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)$$

其中，$\mathcal{F}$是假设空间

当样本容量足够大时，经验风险最小化能保证有很好的学习效果。
**极大似然估计** maximum likelihood estimation 就是经验等闲最小化的一个例子

当样本容量很小时，经验风险最小化学习的效果就未必很好，会产生过拟合 over-fitting 现象

结构风险最小化structual risk minimizatrion ,SRM 是为了防止过拟合而提出来的策略，结构风险最小化是等价于正则化 regularization

结构风险再经验风险上加上表示模型复杂度的正则化项 regularizer 或 罚项 penalty term

在假设空间，损失函数以及训练数据确定的情况下，**结构风险**的定义是
$$R_{\mathrm{srm}}(f)=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$

其中 $J(f)$ 为模型的复杂度，是定义在假设空间 $\mathcal{F}$ 上的泛函, 模型越复杂，复杂度就越大。反之，模型 $f$ 越简单复杂度$J(f)$就越小。 也就是说，复杂度是对复杂函数的惩罚。$\lambda \geqslant 0$ 是系数，用以权衡经验风险和模型复杂度。

贝叶斯估计中的最大后验概率估计 maximum posterior probability estimation MAP 就是结构风险最小化的一个例子。

结构风险最小化的策略认为结构风险最小的模型是最优的模型，所以求最优模型，就是求解最优化问题：
$$\min _{f \in \mathcal{F}} \frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)+\lambda J(f)$$

这样，监督学习问题就变成了经验风险或结构风险函数的最优化问题。这时经验或结构风险函数是最优化的目标函数。